% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/call_llm.R
\name{call_llm}
\alias{call_llm}
\title{Call LLM API with a prompt}
\usage{
call_llm(
  user_prompt,
  system_prompt,
  api_url = Sys.getenv("LLM_API_URL", "http://192.168.10.22:1234/v1/chat/completions"),
  model = Sys.getenv("LLM_MODEL", "openai/gpt-oss-120b"),
  temperature = 0.1
)
}
\arguments{
\item{user_prompt}{Character string. The user's prompt/question to send to the LLM.}

\item{system_prompt}{Character string. System-level instructions that
define the AI's role and behavior.}

\item{api_url}{Character string. The API endpoint URL. Defaults to environment
variable LLM_API_URL or local LM Studio endpoint.}

\item{model}{Character string. The model identifier to use. Defaults to
environment variable LLM_MODEL or "openai/gpt-oss-120b".}

\item{temperature}{Numeric. Controls randomness in the response. 0 = deterministic,
1 = maximum randomness. Default is 0.1 for mostly consistent responses.}
}
\value{
A list containing the full API response, including:
\describe{
\item{choices}{List of response choices, typically one}
\item{usage}{Token usage statistics}
\item{model}{Model used for generation}
\item{id}{Unique identifier for this completion}
}
The actual text response is in \code{result$choices[[1]]$message$content}
}
\description{
Sends a text prompt to a Large Language Model API and returns the response.
This is a minimal wrapper around the OpenAI-compatible chat completions endpoint.
}
\examples{
\dontrun{
# Basic usage
response <- call_llm("What is intimate partner violence?")
content <- response$choices[[1]]$message$content

# With system and user prompts
sys_prompt <- "You are a forensic analyst. Respond only with JSON."
user_prompt <- "Analyze this narrative for IPV indicators..."
response <- call_llm(user_prompt, system_prompt = sys_prompt)

# With custom temperature
response <- call_llm("Analyze this narrative...", temperature = 0)
}

}
\seealso{
\url{https://platform.openai.com/docs/api-reference/chat/create}
}
