% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/store_llm_result.R
\name{store_llm_results_batch}
\alias{store_llm_results_batch}
\title{Batch store multiple LLM results}
\usage{
store_llm_results_batch(
  parsed_results,
  db_path = "llm_results.db",
  chunk_size = NULL,
  conn = NULL
)
}
\arguments{
\item{parsed_results}{List of parsed results}

\item{db_path}{Database file path for SQLite or config for unified connection (default: "llm_results.db")}

\item{chunk_size}{Number of records per transaction (default: 1000 for SQLite, 5000 for PostgreSQL)}

\item{conn}{Existing connection (optional, improves performance)}
}
\value{
List with summary statistics
}
\description{
Efficient batch insertion with transaction support.
Optimized for both SQLite and PostgreSQL backends.
Realistic performance: ~100-500 records/second depending on network latency.
}
\examples{
\dontrun{
# Process multiple narratives and batch store
narratives <- c("Text 1", "Text 2", "Text 3")
parsed_results <- list()

for (i in seq_along(narratives)) {
  response <- call_llm(narratives[i], "System prompt")
  parsed_results[[i]] <- parse_llm_result(response, 
                                          narrative_id = paste0("case_", i))
}

# Batch store all results
batch_result <- store_llm_results_batch(parsed_results)
cat("Inserted:", batch_result$inserted, "\n")
cat("Duplicates:", batch_result$duplicates, "\n") 
cat("Success rate:", round(batch_result$success_rate * 100, 1), "\%\n")

# Custom chunk size for large batches
large_batch_result <- store_llm_results_batch(parsed_results, 
                                             chunk_size = 500)

# Using existing connection for multiple batches
conn <- get_db_connection()
ensure_schema(conn)

batch1 <- store_llm_results_batch(parsed_results[1:10], conn = conn)
batch2 <- store_llm_results_batch(parsed_results[11:20], conn = conn)

close_db_connection(conn)
}
}
