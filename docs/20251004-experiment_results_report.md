# Completed Experiment Results — 2025-10-04

Total completed runs: **27**

| Exp ID | Name | Prompt | Temp | Reasoning | N Proc | F1 | Precision | Recall | Accuracy | Runtime (s) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| c6eb23c0 | Indirect Indicators (T=0.2, Reasoni… | v0.3.2_indicators | 0.2 | high | 400 | 0.808 | 0.750 | 0.875 | 0.950 | 2060 |
| 7bca6b0d | Indicators v0.4.2 (T=0.0, Medium) | v0.4.2_indicators | 0.0 | medium | 404 | 0.784 | 0.741 | 0.833 | 0.946 | 1828 |
| 3366e3ff | Indirect Indicators (T=0.0, Reasoni… | v0.3.2_indicators | 0.0 | medium | 399 | 0.772 | 0.722 | 0.830 | 0.942 | 1931 |
| ed486757 | Indirect Indicators (T=0.0, Reasoni… | v0.3.2_indicators | 0.0 | high | 400 | 0.772 | 0.736 | 0.812 | 0.943 | 2089 |
| 37c36c33 | Baseline + Victim Check (T=0.0, Rea… | v0.3.1_baseline | 0.0 | high | 402 | 0.763 | 0.755 | 0.771 | 0.943 | 2073 |
| c3dc851a | Full Context (T=0.2, Reasoning=High) | v0.3.4_context | 0.2 | high | 404 | 0.759 | 0.846 | 0.688 | 0.948 | 2158 |
| f87d8410 | Baseline + Victim Check (T=0.2, Rea… | v0.3.1_baseline | 0.2 | high | 399 | 0.753 | 0.778 | 0.729 | 0.942 | 2071 |
| 60376368 | Test GPT-OSS-120B new prompt 2025-1… | v0.2.1_April | 0.2 | - | 404 | 0.745 | 0.761 | 0.729 | 0.941 | 1590 |
| 788b7a4a | Baseline v0.4.1 (T=0.0, High) | v0.4.1_baseline | 0.0 | high | 396 | 0.744 | 0.821 | 0.681 | 0.944 | 1862 |
| f7b8fe62 | Baseline v0.4.1 (T=0.2, High) | v0.4.1_baseline | 0.2 | high | 404 | 0.739 | 0.773 | 0.708 | 0.941 | 1921 |
| 511dc2f6 | Test mlx-community/gpt-oss-120b wit… | v0.2.2_Xiaosong | 0.0 | - | 404 | 0.733 | 0.786 | 0.688 | 0.941 | 1602 |
| d0917746 | Baseline v0.4.1 (T=0.0, Medium) | v0.4.1_baseline | 0.0 | medium | 404 | 0.731 | 0.756 | 0.708 | 0.938 | 1753 |
| cfb799ca | Full Context (T=0.0, Reasoning=High) | v0.3.4_context | 0.0 | high | 401 | 0.729 | 0.838 | 0.646 | 0.943 | 2174 |
| ddd5746b | CoT + Examples (T=0.0, Reasoning=Hi… | v0.4.0_cot_examples | 0.0 | high | 404 | 0.713 | 0.795 | 0.646 | 0.938 | 3103 |
| 143be19c | Baseline + Victim Check (T=0.0, Rea… | v0.3.1_baseline | 0.0 | medium | 402 | 0.703 | 0.744 | 0.667 | 0.933 | 3011 |
| cf1eba57 | Full Context (T=0.0, Reasoning=Medi… | v0.3.4_context | 0.0 | medium | 404 | 0.699 | 0.829 | 0.604 | 0.938 | 1985 |
| f272df65 | CoT + Examples (T=0.2, Reasoning=Hi… | v0.4.0_cot_examples | 0.2 | high | 404 | 0.682 | 0.784 | 0.604 | 0.933 | 2874 |
| e21c425c | CoT + Examples (T=0.0, Reasoning=Me… | v0.4.0_cot_examples | 0.0 | medium | 404 | 0.667 | 0.818 | 0.562 | 0.933 | 2811 |
| d53afb75 | CoT + Examples (T=0.0, Reasoning=Hi… | v0.4.0_cot_examples | 0.0 | high | 404 | 0.659 | 0.794 | 0.562 | 0.931 | 2869 |
| 93641da2 | CoT + Examples (T=0.0, Reasoning=Me… | v0.4.0_cot_examples | 0.0 | medium | 270 | 0.633 | 0.704 | 0.576 | 0.919 | 2180 |
| 3c983f41 | Test qwen/qwen3-next-80b with modif… | v0.2.2_Xiaosong | 0.0 | - | 404 | 0.581 | 0.430 | 0.896 | 0.847 | 768 |
| 1dcaea4c | Strict Criteria + Self-Defense (T=0… | v0.3.3_strict | 0.2 | high | 404 | 0.580 | 0.952 | 0.417 | 0.928 | 1959 |
| 5f0cf661 | Strict Criteria + Self-Defense (T=0… | v0.3.3_strict | 0.0 | high | 404 | 0.563 | 0.870 | 0.417 | 0.923 | 1986 |
| 6d93b22c | Strict Criteria + Self-Defense (T=0… | v0.3.3_strict | 0.0 | medium | 404 | 0.543 | 0.864 | 0.396 | 0.921 | 1870 |
| 9770be9b | Test GPT-OSS-120B with High Reasoni… | v0.3.0_reasoning_Xi… | 0.0 | high | 403 | 0.529 | 0.857 | 0.383 | 0.921 | 3103 |
| bc9af9de | Test qwen/qwen3-next-80b new prompt… | v0.2.1_April | 0.2 | - | 404 | 0.503 | 0.344 | 0.938 | 0.780 | 755 |
| cc5ab818 | Test GPT-OSS-120B Baseline | v2.1_andrea | 0.1 | - | 10 | - | - | - | 1.000 | 61 |

*Reasoning level is parsed from the first "Reasoning:" line in the system prompt.*

## Observations and Strategy Notes

- **Indicator-focused prompts (v0.3.2/v0.4.2)** dominate the top of the leaderboard. Medium reasoning already performs well, while combining high reasoning with a slightly warmer temperature (0.2) produces the current best F1 (0.808).
- **Baseline v0.4.1** remains a reliable fallback: precision stays high, and F1 hovers around 0.74, though it trails the tuned indicator prompts by ~0.05.
- **Reasoning level trade-offs:** medium reasoning keeps runs deterministic, while high reasoning boosts precision at the cost of longer runtimes. The effect is visible across both indicator and baseline families.
- **Temperature sweeps:** raising temperature to 0.2 generally improves recall for the indicator prompt but slightly hurts recall on the baseline prompt; pick based on whether recall or precision is the priority.
- **Strict-criteria prompts** achieve very high precision (>0.95) but their F1 slips into the 0.56–0.58 range because recall drops sharply—useful when minimizing false positives matters more than overall balance.

These patterns mirror the earlier prompt/strategy assessment: focus on the indicator prompt family for balanced F1, keep the refined baseline prompt when you need stable precision, and reserve strict criteria runs for audits or false-positive reviews.

