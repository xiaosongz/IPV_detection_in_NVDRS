# Enhanced IPV Detection Test Framework

This comprehensive testing framework provides robust evaluation capabilities for the IPV detection system, including statistical analysis, visualization, and performance tracking.

## Overview

The framework consists of four main components:

1. **Enhanced Database Schema** (`schema/enhanced_test_tracking_schema.sql`) - Complete tracking system
2. **Core Test Framework** (`R/test_framework.R`) - Test execution and metrics calculation
3. **Visualization Suite** (`R/visualization_utils.R`) - Comprehensive plotting functions
4. **Statistical Testing** (`R/statistical_tests.R`) - Significance tests and A/B testing

## Directory Structure

```
tests/
â”œâ”€â”€ README.md                                # This comprehensive documentation
â”œâ”€â”€ schema/
â”‚   â””â”€â”€ enhanced_test_tracking_schema.sql    # Database schema with 11 specialized tables
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ performance_analysis_queries.sql     # 10 key analysis queries
â”œâ”€â”€ R/
â”‚   â”œâ”€â”€ test_framework.R                     # Core testing functions & batch processing
â”‚   â”œâ”€â”€ visualization_utils.R                # 7 visualization functions
â”‚   â””â”€â”€ statistical_tests.R                  # McNemar's, bootstrap, A/B testing
â”œâ”€â”€ integration_tests/                       # Original integration tests
â”‚   â””â”€â”€ test_real_data.R                     # Script to test with real NVDRS data
â”œâ”€â”€ test_data/                               # Test input data and configurations
â”‚   â”œâ”€â”€ test_config.yml                      # Test configuration file
â”‚   â””â”€â”€ test_sample.csv                      # Sample test data (20 records)
â”œâ”€â”€ test_results/                            # Basic test output
â”‚   â”œâ”€â”€ test_results.csv                     # Detection results from test run
â”‚   â””â”€â”€ test_report.txt                      # Detailed test report
â”œâ”€â”€ reports/                                 # Generated visualizations & reports
â”œâ”€â”€ results/                                 # Comprehensive test outputs
â””â”€â”€ example_usage.R                          # Complete usage examples with 6 scenarios
```

## Key Features

### ðŸ“Š Performance Tracking
- Confusion matrices with detailed breakdowns
- ROC curves and AUC calculations  
- Confidence score calibration analysis
- Bootstrap confidence intervals
- A/B testing with statistical significance

### ðŸ“ˆ Advanced Analytics
- Error pattern analysis and classification
- Indicator effectiveness measurement
- Narrative quality vs accuracy correlation
- Temporal performance trends
- Cost-benefit analysis (API costs vs performance)

### ðŸ”¬ Statistical Rigor
- McNemar's test for paired model comparisons
- Bootstrap resampling for confidence intervals
- Power analysis for sample size planning
- Effect size calculations (Cohen's d, odds ratios)
- Multiple comparison corrections

### ðŸ“‹ Enhanced Database Schema

The enhanced schema includes 11 specialized tables:

| Table | Purpose |
|-------|---------|
| `test_runs` | Test run metadata and configuration tracking |
| `prompt_versions` | Version control for prompts and templates |
| `test_results` | Individual prediction results with confidence scores |
| `performance_metrics` | Aggregated accuracy, precision, recall, F1 scores |
| `error_analysis` | Detailed misclassification pattern analysis |
| `confidence_calibration` | Confidence score reliability assessment |
| `indicator_analysis` | Effectiveness of specific IPV indicators |
| `narrative_quality` | Narrative completeness vs accuracy correlation |
| `ab_tests` | A/B test comparisons with statistical results |

## Usage Examples

### Quick Start
```r
# Load the framework
source("tests/R/test_framework.R")
source("tests/R/visualization_utils.R")

# Run a basic test
conn <- init_test_database("tests/test_logs.sqlite")
test_data <- readr::read_csv("tests/test_data/test_sample.csv")
config <- nvdrsipvdetector::load_config("config/settings.yml")

results <- run_ipv_detection_test(
  test_data = test_data,
  config = config,
  run_name = "Baseline Test",
  conn = conn
)

# Generate comprehensive report
plots <- generate_test_report(conn, results$run_id, "tests/reports")
```

### A/B Testing
```r
# Compare two configurations
ab_test_id <- create_ab_test(
  conn = conn,
  test_name = "Prompt v1.0 vs v2.0",
  variant_a_run_id = "run_20241201_baseline",
  variant_b_run_id = "run_20241201_improved", 
  test_metric = "f1_score"
)

# Statistical significance testing
mcnemar_result <- mcnemar_test_comparison(conn, run_a, run_b, "combined")
```

### Performance Analysis
```r
# Bootstrap confidence intervals
ci_results <- bootstrap_performance_ci(
  conn, run_id, 
  metric = "f1_score", 
  narrative_type = "combined",
  n_bootstrap = 1000
)

# Visualization suite
plot_confusion_matrix(conn, run_id, "combined")
plot_roc_curve(conn, run_id, "combined") 
plot_confidence_calibration(conn, run_id, "combined")
```

## Key SQL Queries

### Performance Comparison
```sql
SELECT 
    tr.run_name,
    pm.narrative_type,
    ROUND(pm.accuracy, 4) as accuracy,
    ROUND(pm.f1_score, 4) as f1_score,
    pm.true_positives,
    pm.false_positives,
    pm.false_negatives
FROM test_runs tr
JOIN performance_metrics pm ON tr.run_id = pm.run_id
WHERE tr.status = 'completed'
ORDER BY tr.run_timestamp DESC;
```

### Error Pattern Analysis
```sql
SELECT 
    error_type,
    narrative_type,
    misclassification_reason,
    COUNT(*) as occurrence_count,
    AVG(predicted_confidence) as avg_confidence
FROM error_analysis 
WHERE run_id = ?
GROUP BY error_type, narrative_type, misclassification_reason
HAVING COUNT(*) >= 2
ORDER BY occurrence_count DESC;
```

### Confidence Calibration
```sql
SELECT 
    bin_center,
    prediction_count,
    accuracy_in_bin,
    calibration_error,
    CASE 
        WHEN calibration_error < 0.05 THEN 'Well Calibrated'
        WHEN calibration_error < 0.10 THEN 'Moderately Calibrated'
        ELSE 'Poorly Calibrated'
    END as calibration_quality
FROM confidence_calibration 
WHERE run_id = ?
ORDER BY bin_center;
```

## Recommended Test Strategies

### 1. Baseline Establishment
- **Sample Size**: 500-1000 cases minimum for stable metrics
- **Stratification**: Balance by narrative type (LE/CME) and IPV prevalence
- **Metrics**: Focus on F1-score as primary metric (balances precision/recall)

### 2. Prompt Engineering Validation
- **A/B Testing**: Compare prompt versions with â‰¥200 cases per variant
- **Statistical Power**: Target 80% power to detect 5% F1-score improvements  
- **Multiple Testing**: Apply Bonferroni correction for multiple comparisons

### 3. Model Comparison
- **Cross-validation**: 5-fold CV for robust performance estimation
- **Bootstrap CI**: 1000 samples for 95% confidence intervals
- **Effect Sizes**: Report Cohen's d for practical significance

### 4. Error Analysis Protocol
- **Misclassification Patterns**: Manual review of high-confidence errors
- **Indicator Analysis**: Track which IPV indicators are most predictive
- **Narrative Quality**: Correlate completeness scores with accuracy

### 5. Performance Monitoring
- **Trend Analysis**: Track performance over time to detect drift
- **Calibration Checks**: Ensure confidence scores remain well-calibrated
- **Cost Analysis**: Monitor API costs vs performance improvements

## Statistical Guidelines

### Sample Size Recommendations
| Effect Size | Description | Cases per Group |
|-------------|-------------|----------------|
| 0.1 (1-2%) | Very small improvement | 1571 |
| 0.2 (3-5%) | Small improvement | 393 |
| 0.3 (5-8%) | Medium improvement | 175 |
| 0.5 (10%+) | Large improvement | 64 |

### Significance Testing
- **Primary Test**: McNemar's test for paired accuracy comparisons
- **Alpha Level**: 0.05 with Bonferroni correction for multiple tests
- **Confidence Intervals**: Report 95% bootstrap CIs for all metrics
- **Effect Sizes**: Always report alongside p-values

### Quality Thresholds
- **Accuracy**: â‰¥85% for production deployment
- **F1-Score**: â‰¥80% with balanced precision/recall
- **Calibration ECE**: <0.10 for well-calibrated confidence scores
- **Processing Time**: <2 seconds per narrative on average

## Running Tests

### Enhanced Framework Tests
```r
# Load enhanced framework
source("tests/R/test_framework.R")
source("tests/R/visualization_utils.R")
source("tests/R/statistical_tests.R")

# Run comprehensive example
source("tests/example_usage.R")
results <- run_all_examples()

# Quick start for immediate testing
quick_results <- quick_start_test()
```

### Original Unit & Integration Tests
```r
# Run all package unit tests
devtools::test()

# Run integration test with real data
source("tests/integration_tests/test_real_data.R")
```

## Integration with Existing System

The test framework integrates seamlessly with the existing `nvdrsipvdetector` package:

```r
# Use existing functions with test framework
result <- nvdrsipvdetector::detect_ipv(narrative, "LE", config, conn)

# Batch processing with enhanced logging
results <- nvdrsipvdetector::nvdrs_process_batch(data, config, validate = TRUE)

# Connect to test database for analysis
conn <- init_test_database()
record_test_results(conn, run_id, results)
```

## Troubleshooting

### Common Issues
1. **Insufficient Data**: Ensure â‰¥50 cases per narrative type for meaningful analysis
2. **API Timeouts**: Implement exponential backoff and retry logic
3. **Memory Issues**: Process in batches of 50-100 cases
4. **Database Locks**: Use WAL mode for better concurrent access

### Performance Optimization  
1. **Batch Processing**: Optimal batch size is typically 25-50 cases
2. **Database Indexing**: All performance-critical queries are indexed
3. **Parallel Processing**: Consider parallel API calls with rate limiting
4. **Caching**: Implement caching for duplicate narratives

## Test Coverage Summary
- **Legacy Tests**: 97 passing unit tests, 3 skipped (API-dependent)
- **Integration Tests**: Successfully processes real NVDRS data  
- **Enhanced Framework**: 11 database tables, 10 analysis queries, 7 visualizations
- **Statistical Tests**: McNemar's, bootstrap CI, A/B testing, power analysis
- **Mock Validation**: 55% accuracy with keyword-based mock LLM

## Future Enhancements

1. **Real-time Monitoring**: Dashboard for live performance tracking
2. **Automated A/B Testing**: Continuous experimentation framework  
3. **Advanced ML Metrics**: ROC-AUC, PR-AUC for imbalanced datasets
4. **Explainability Analysis**: SHAP/LIME integration for interpretability
5. **Bias Detection**: Fairness metrics across demographic groups

This framework provides production-ready testing infrastructure for systematic evaluation and improvement of the IPV detection system.